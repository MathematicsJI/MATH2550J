\documentclass[aspectratio=169, UTF8]{ctexbeamer}
\usepackage{math214}

\usepackage{babel}
\usepackage{bm}
\usepackage{geometry}


%\usepackage{enumitem}
% Then, after \begin{document},  you can begin your frames/slides

\title{\LARGE 255RC2}
\author{Chengyang Shi, Haoran Shen, Jingfan Tang, Ruizhi Deng}
\date{Summer 2025}

\definecolor{darkblue}{HTML}{6666dd} 
\colortheme{green!30!black}
%\colortheme{orange!85!black}
%\colortheme{darkblue}
%\colortheme{blue!100!black}
%\colortheme{orange!85!white!90!black}
\begin{document}



\maketitle


\begin{frame}
    \frametitle{Contents}
    \tableofcontents     % 生成目录
\end{frame}






% Example frame
\section{Transpose}
\begin{frame}{Transpose}
    \begin{block}{Definition}
        $\text{Given } A = (a_{ij}), A^T := (a_{ji})$
    \end{block}

    \begin{block}{Proposition}
        Let $A, B$ be matrices, $\lambda \in \mathbb{R}$.
        \begin{enumerate}
            \item $(A+B)^T = A^T + B^T$, $(\lambda A)^T = \lambda A^T $
            \item $(AB)^T = B^T A^T$
            \item $(A^T)^T = A$
            \item If $A$ is invertible, so is $A^T$, and $(A^T)^{-1} = (A^{-1})^T = A^{-T}$
        \end{enumerate}
    \end{block}
\end{frame}
\begin{frame}{Symmetric Matrix}
    \begin{block}{Definition}
        A matrix $A$ is called \textbf{symmetric} if $A = A^T$

        A matrix $A$ is called \textbf{skew-symmetric} if $A^T = -A \iff A + A^T = 0$
    \end{block}
    \begin{block}{Remark}
        \begin{enumerate}
            \item A symmetric or skew-symmetric matrix is necessarily square.
            \item For a skew-symmetric matrix, if $i=j$, then $a_{ij} = 0$
            \item Any $n \times n$ diagonal matrix is symmetric.
        \end{enumerate}
    \end{block}
    \begin{block}{Example}
        $A = \begin{pmatrix} 1 & -4 & -2 \\ 4 & 1 & 3 \\ 2 & -3 & 2 \end{pmatrix} \text{is not skew-symmetric.}$
    \end{block}
\end{frame}
\section{Matrix as a Function}
\begin{frame}{Matrix as a Function}
    \begin{block}{Definition}
        An $ n \times m$ Matrix can be interpreted as a function $F: M_{m \times p} \to M_{n \times p}$
    \end{block}
    \begin{block}{Property}
        For $M \in M_{n \times m}, A,B \in M_{m \times p}, \alpha , \beta \in \mathbb{R}$ \\
        We have $M(\alpha A + \beta B) = \alpha MA + \beta MB$
    \end{block}
    \begin{block}{Question}
        \begin{enumerate}
            \item Do you know other function with such property?
            \item Why we want such property?
        \end{enumerate}
    \end{block}
\end{frame}
\section{Orthogonal Matrices and Orthonormal Vectors}
\begin{frame}{Orthogonal Matrix}
    \begin{block}{Definition}
        A matrix $A \in M_n(\mathbb{R})$ is called an orthogonal matrix iff $A^T = A^{-1}$
    \end{block}
    \begin{block}{Properties.}
        \begin{itemize}
            \item $I_n$ is an orthogonal matrix.
            \item If A and B are orthogonal matrices, then AB is also orthogonal.
            \item If A is an orthogonal matrix, then $A^{-1}$ is also orthogonal matrix.
            \item If A is an orthogonal matrix, then $det(A)= \pm 1$.
            \item If A is an orthogonal matrix, then $AA^T = A^T A = I_n$.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Orthogonal Matrix}

    \begin{block}{Examples.}
        {$\blacktriangleright$ Rotation Matrix.}
        \par Rotation by $\theta$ in $\mathbb{R}^2$ is given by
        \begin{equation*}
            \left[ \begin{array}{cc}
                    \cos \theta & -\sin \theta \\
                    \sin \theta & \cos \theta  \\
                \end{array} \right].
        \end{equation*}

        {$\blacktriangleright$ Reflection Matrix.}
        \par Reflect $(x_1,x_2)$ across $\theta / 2$ in $\mathbb{R}^2$ is given by
        \begin{equation*}
            \left[ \begin{array}{cc}
                    \cos \theta & \sin \theta  \\
                    \sin \theta & -\cos \theta \\
                \end{array} \right].
        \end{equation*}
    \end{block}

\end{frame}

\begin{frame}{Demo}
    You can treat such matrix as a function $Q:$ $\mathbb{R}^2 \to \mathbb{R}^2$
    \begin{figure}
        \centering
        \includegraphics[width=1\linewidth]{demo_rotate&reflect.png}
        \label{fig:enter-label}
    \end{figure}
\end{frame}
\begin{frame}{Orthogonal Vectors and Orthonormal Vectors}
    You may need to distinguish these two concepts clearly \dots
    \begin{block}{Definition}
        Let $\{v_1,\cdots,v_k\} \in \mathbb{R}$ be a subset of $\mathbb{R}^k$ with $k$ distinct vectors, then $\{v_1,\cdots,v_k\}$ is an \textcolor{red}{orthogonal set of vectors} if $\langle v_i, v_j \rangle = 0$ for all $1 \leq i,j \leq k$, $i \neq j$.

        Also, $\{q_1,\cdots,q_k\}$ is an \textcolor{red}{orthonormal set of vectors} if it is an orthogonal set and all of its vectors are unit vectors (\textit{i.e.}, $\|q_i\| = 1$ for $i \leq i \leq k$).

    \end{block}

    \begin{block}{Remark}
        Any set containing a single vector is orthogonal; any set containing a single unit vector is orthonormal.
    \end{block}

    \begin{block}{Example.} $\hat{i},\hat{j},\hat{k} \in \mathbb{R}^3$.
    \end{block}

\end{frame}

\begin{frame}{Orthogonal Vectors and Orthonormal Vectors}
    \begin{block}{Proposition}
        Given vectors $q_i \in \mathbb{R}^n$, $i = 1,\cdots,m$ such that
        \begin{equation*}
            \langle q_i , q_j \rangle = q_i^T q_j = \delta_{ij} = \left\{ \begin{array}{ll}
                \mathbf{1}, & i = j,    \\
                0,          & i \neq j, \\
            \end{array} \right.
            (\delta_{ij} : Kronecker)
        \end{equation*}
        then we call the set of vectors $\{q_i\}$ \textbf{orthonormal}, and
        \begin{equation*}
            Q=\left[\begin{array}{ccc}
                    \mid  &        & \mid  \\
                    q_{1} & \ldots & q_{m} \\
                    \mid  &        & \mid
                \end{array}\right] \Rightarrow Q^TQ = I_{m}.
        \end{equation*}
    \end{block}

\end{frame}

\begin{frame}{Orthogonal Vectors and Orthonormal Vectors}
    \begin{block}{Proof}
        \begin{equation*}
            \begin{aligned}
                Q^{T} Q & =\left[\begin{array}{ccc}
                                         - & q_{1}^{T} & - \\
                                           & \vdots    &   \\
                                         - & q_{m}^{T} & -
                                     \end{array}\right]\left[\begin{array}{ccc}
                                                                 \mid  &        & \mid  \\
                                                                 q_{1} & \ldots & q_{m} \\
                                                                 \mid  &        & \mid
                                                             \end{array}\right]=\left[\begin{array}{ccc}
                                                                                          q_{1}^{T} q_{1} & \ldots & q_{1}^{T} q_{m} \\
                                                                                          \vdots          & \ddots & \vdots          \\
                                                                                          q_{m}^{T} q_{1} & \ldots & q_{m}^{T} q_{m}
                                                                                      \end{array}\right] =I_{m}
            \end{aligned}
        \end{equation*}
    \end{block}
\end{frame}

\begin{frame}{Orthogonal Matrices and Orthonormal Vectors}
    \begin{block}{Proposition}
        $\text{Let } Q = \begin{bmatrix} | & | & & | \\ \mathbf{q}_1 & \mathbf{q}_2 & \cdots & \mathbf{q}_n \\ | & | & & | \end{bmatrix} \in M_n(\mathbb{R})$,
        where $\{\mathbf{q}_1, \mathbf{q}_2, \dots, \mathbf{q}_n \}$ is a set of \textbf{orthonormal} vectors in $\mathbb{R}^n.$\\

        Then $Q$ is an orthogonal matrix, $Q^T = Q^{-1}$.\\
    \end{block}
    \begin{block}{Proof}
        We verify by showing $QQ^T = I_n$ and $Q^T Q = I_n$ holds.
    \end{block}
    \textcolor{red}{To show matrix $B$ is the inverse of matrix $A$, we need to show \textbf{both} $AB = I$ and $BA = I$.}
\end{frame}


\begin{frame}{Exercises}

    \par \textcolor{red}{Exercise}
    \par Determine whether the following matrices are orthogonal matrices
    \begin{equation*}
        \left[ \begin{array}{cc}
                \frac{\sqrt{3}}{2} & -\frac{1}{2}       \\
                \frac{1}{2}        & \frac{\sqrt{3}}{2} \\
            \end{array} \right].
    \end{equation*}

    \begin{equation*}
        \left[ \begin{array}{cc}
                1 & 1  \\
                1 & -1 \\
            \end{array} \right].
    \end{equation*}
\end{frame}
\begin{frame}{Orthogonal Matrices and Orthonormal Vectors}
    \begin{block}{Proposition}
        The orthogonal matrices are precisely the matrices that preserve the inner product in $\mathbb{R}^n$ \\
        i.e., $\forall x,y \in \mathbb{R}^n$, $\langle x,y \rangle = \langle Ax, Ay \rangle$
        or $x^T y = (Ax)^T Ay$.
    \end{block}



    \begin{proof}
        If $A$ is orthogonal, i.e., $A^{-1} = A^T$,
        then $A^T A = I_n$, so $(Ax)^T (Ay) = x^T A^T A y = x^T I_n y = x^T y$.
    \end{proof}
\end{frame}
\begin{frame}
    \frametitle{Preserve Inner Product}
    \begin{definition}[Inner Product Preservation]
        一个矩阵$Q$保持内积不变，意味着对任意两个向量$ \mathbf{u}, \mathbf{v} \in \mathbb{R}^n $，应用$Q$ 变换后，它们的内积保持不变，即：
        \begin{equation*}
            \langle Q\mathbf{u}, Q\mathbf{v} \rangle = \langle \mathbf{u}, \mathbf{v} \rangle
        \end{equation*}
    \end{definition}
    \begin{block}{几何意义}
        \begin{itemize}
            \item \textbf{长度不变}：由于向量范数为
                  \[
                      \|\mathbf{u}\| = \sqrt{\mathbf{u} \cdot \mathbf{u}},
                  \]
                  我们有：
                  \[
                      \|Q\mathbf{u}\| = \sqrt{(Q\mathbf{u}) \cdot (Q\mathbf{u})} = \sqrt{\mathbf{u} \cdot \mathbf{u}} = \|\mathbf{u}\|.
                  \]
                  因此，正交矩阵保持向量的长度不变。

        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Preserve Inner Product}
    \begin{block}{几何意义}
        保持内积不变意味着矩阵 $Q$ 代表的线性变换保留了 $\mathbb{R}^n$ 的几何结构，具体包括：

        \begin{itemize}
            \item \textbf{角度不变}：内积决定了两向量夹角 $\theta$，因为
                  \[
                      \mathbf{u} \cdot \mathbf{v} = \|\mathbf{u}\| \|\mathbf{v}\| \cos(\theta).
                  \]
                  保持内积不变意味着 $\mathbf{u}$ 和 $\mathbf{v}$ 的夹角与 $Q\mathbf{u}$ 和 $Q\mathbf{v}$ 的夹角相同。

            \item \textbf{正交性不变}：如果 $\mathbf{u}$ 和 $\mathbf{v}$ 垂直（即 $\mathbf{u} \cdot \mathbf{v} = 0$），那么
                  \[
                      (Q\mathbf{u}) \cdot (Q\mathbf{v}) = 0,
                  \]
                  即 $Q\mathbf{u}$ 和 $Q\mathbf{v}$ 也垂直。
        \end{itemize}
    \end{block}
\end{frame}


\begin{frame}{Why we need Orthonormal Matrix?}
    \begin{enumerate}
        \item \textbf{Magnitude of Determinant and Column Norms:}
              \begin{itemize}
                  \item $|\det(A)| = 1$, where $A$ is an orthogonal matrix.
                  \item Column vectors $\bm{q}_i$ have unit norm: $||\bm{q}_i|| = 1$.
                  \item \textit{Implication:} The transformation represented by $A$ is a \textbf{rotation} or a \textbf{reflection}.
              \end{itemize}

        \item \textbf{Defining Property and Its Consequences:}
              \begin{itemize}
                  \item $A^T A = I_n$ (which means $A^{-1} = A^T$).
                  \item The original note mentions this "has very good symmetric properties."
                  \item \textit{Implications:}
                        \begin{itemize}
                            \item Preservation of inner products (hence, lengths and angles are preserved: $\langle Ax, Ay \rangle = \langle x, y \rangle$).
                            \item The inverse $A^{-1}$ is easy to compute (it's just the transpose $A^T$).
                        \end{itemize}
              \end{itemize}

        \item \textbf{Applications in Data Processing:}
              \begin{itemize}
                  \item Effective in removing redundant information from data
              \end{itemize}
    \end{enumerate}
\end{frame}

\section{Kernel and Image}
\begin{frame}{Kernel and Image}
    \begin{block}{Definition}
        \begin{itemize}
            \item Given matrix $A \in M_{m \times n}(\mathbb{R})$. The kernel, or nullspace of $A$, is defined as
                  \[
                      \operatorname{ker} A = N(A) = \{x \in \mathbb{R}^n \mid Ax = \mathbf{0}\} \subseteq \mathbb{R}^n
                  \]
                  where $A: \mathbb{R}^n \rightarrow \mathbb{R}^m$.

                  $\rightarrow$ "Those inputs s.t. the output is 'zero'."

            \item The image of $A$, or column space, is defined as
                  \begin{align*}
                      \operatorname{im} A = C(A) & = \{y \in \mathbb{R}^m \mid \exists x \in \mathbb{R}^n, y = Ax\} \\
                                                 & = \{Ax \mid x \in \mathbb{R}^n\}
                  \end{align*}
                  where $A: \mathbb{R}^n \rightarrow \mathbb{R}^m$.

                  $\rightarrow$ "The set of all possible outputs"
                  \newline
                  (The image is a subspace of $\mathbb{R}^m$).
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Injectivity}
    \begin{block}{Definition}
        A function $f: A \to B$ is called \textbf{injective}, if \\
        $\forall x_1, x_2 \in A, f(x_1) = f(x_2) \implies x_1 = x_2$ \\
        i.e., $\forall x_1, x_2 \in A, x_1 \neq x_2 \implies f(x_1) \neq f(x_2)$
        \\
        $\bullet$ 输入与输出一一对应
    \end{block}
    \begin{block}{$\bigstar$ Proposition}
        Given $A \in M_{m \times n}(\mathbb{R})$, i.e., $A: \mathbb{R}^n \to \mathbb{R}^m$ \\
        then, \textcolor{red}{$A$ is injective $\iff$ $\ker A = N(A) = \{0\}$}
    \end{block}
\end{frame}

\begin{frame}{Injectivity}
    \begin{block}{Proof}
        ($\Rightarrow$) A is injective$\;\implies\forall x_1, x_2 \in \mathbb{R}^n, Ax_1 = Ax_2 \implies x_1 = x_2$.

        Take $v \in \ker A$, i.e., $Av = 0$.
        We also know that $A0 = 0$, hence $Av = A0$.
        By injectivity of A, $v=0$. Therefore $\ker A = \{0\}$.
        \newline

        ($\Leftarrow$) Conversely, we know $\ker A = \{0\}$.
        
        Take $v_1, v_2 \in \mathbb{R}^n$, s.t. $Av_1 = Av_2$.
        We want to show that $v_1 = v_2$.
        Indeed, we have $A(v_1 - v_2) = 0$, but $\ker A = \{0\}$,
        hence $v_1 - v_2 = 0$. So $v_1 = v_2$.
    \end{block}
\end{frame}

\begin{frame}{Kernel and Image}
    \begin{block}{Proposition}
        Given $A \in M_{m \times n}(\mathbb{R})$ , $\ker(A) = \ker(A^T A)$
    \end{block}
    \begin{block}{Proof}
        ($\subseteq$) Take $x \in \ker(A)$, i.e., $Ax=0$.
        So $(A^T A)x = A^T(Ax) = A^T 0 = 0$.
        So $x \in \ker(A^T A)$.

        ($\supseteq$) Take $x \in \ker(A^T A)$, i.e., $A^T Ax = 0$.
        So $x^T A^T Ax = x^T 0 = 0$.
        $(Ax)^T Ax = \|Ax\|^2 = 0$.
        $\implies Ax = 0 \implies x \in \ker A$.
        ($v_1^2 + v_2^2 + \dots + v_n^2 = 0 \implies v_i = 0$)
    \end{block}
\end{frame}

\section{Projection Matrix}
\begin{frame}{Projection Matrix}
    \begin{block}{Definition}
        A matrix $P \in M_n(\mathbb{R})$ is a \textbf{projection matrix}, if $P^2=P$.

        A projection matrix $P$ is called an \textbf{orthogonal projection}, if $P=P^T$.
    \end{block}
    \begin{block}{Example}
        \textbf{For any orthonormal matrix $Q$, $QQ^T: \mathbb{R}^n \to \mathbb{R}^n$ is a projection matrix.}
        \begin{itemize}
            \item $(QQ^T)^T = (Q^T)^T Q^T = Q Q^T$
            \item $(QQ^T)^2 = (QQ^T)(QQ^T) = Q(Q^T Q)Q^T = Q I_k Q^T = QQ^T$
        \end{itemize}
    \end{block}
\end{frame}
\begin{frame}{Orthogonal Projection Matrix}
    \begin{block}{Proposition}
        A projection matrix $P$ is an \textbf{orthogonal projection} if
        $$ \ker P \perp \operatorname{im} P $$
        i.e., $\forall x \in \ker P$, $\forall y \in \operatorname{im} P$,
        $$ \langle x, y \rangle = x^T y = 0 \in \mathbb{R} $$
    \end{block}
    \begin{block}{Exercise}
        Show that $ \ker P \perp \operatorname{im} P $ if $P = P^T$
    \end{block}
\end{frame}
\begin{frame}{Orthogonal Projection Matrix}
    \begin{block}{Example}
        $$
            \{q_1, q_2\} = \left\{
            \begin{pmatrix} 1/\sqrt{2} \\ 1/\sqrt{2} \\ 0 \end{pmatrix},
            \begin{pmatrix} 1/\sqrt{2} \\ -1/\sqrt{2} \\ 0 \end{pmatrix}
            \right\}
        $$

        $$
            Q = \begin{bmatrix}
                1/\sqrt{2} & 1/\sqrt{2}  \\
                1/\sqrt{2} & -1/\sqrt{2} \\
                0          & 0
            \end{bmatrix}, \quad
            Q^T = \begin{bmatrix}
                1/\sqrt{2} & 1/\sqrt{2}  & 0 \\
                1/\sqrt{2} & -1/\sqrt{2} & 0
            \end{bmatrix}
        $$

        $$
            QQ^T = \begin{bmatrix}
                1 & 0 & 0 \\
                0 & 1 & 0 \\
                0 & 0 & 0
            \end{bmatrix}
            \begin{pmatrix} x \\ y \\ z \end{pmatrix}
            =
            \begin{pmatrix} x \\ y \\ 0 \end{pmatrix}
        $$
    \end{block}

\end{frame}


\begin{frame}{Reference}
    \begin{enumerate}
        \item VV255, Lecture Notes 24SU, Runze Cai
        \item VV255 RC2, Jiayue Huang, et al
        \item Introduction to Linear Algebra, Sixth Edition, Gilbert Strang
    \end{enumerate}
\end{frame}

% \begin{frame}{\textcolor{green!30!black}{end}}
%     \begin{center}
%         \LARGE Thank you and Enjoy your Summer Semester!
%     \end{center}
% \end{frame}
\end{document}